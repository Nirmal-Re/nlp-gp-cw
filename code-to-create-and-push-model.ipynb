{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUZ7Nszi5wpH",
        "outputId": "7b6012f0-645c-4bab-e07b-dd4c66bbc8bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U accelerate>=0.21.0\n",
        "!pip install torch -U\n",
        "!pip install transformers -U\n",
        "!pip install datasets spacy tqdm\n",
        "!pip install seqeval\n",
        "!pip install evaluate\n",
        "!pip install pandas seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhSc3P8Y46Qp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# import torchtext\n",
        "\n",
        "SEED = 1234\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "# print(\"torchtext Version: \", torchtext.__version__)\n",
        "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPoyVfq_6T8C"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FccLhiT497Q"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names, ClassLabel, Sequence\n",
        "\n",
        "#description\n",
        "dataset_name = \"surrey-nlp/PLOD-CW\"\n",
        "ds_builder = load_dataset_builder(dataset_name)\n",
        "print(ds_builder.info.description)\n",
        "print(ds_builder.info.features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VySOMhp5D5i"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAC-8PDX6NFP",
        "outputId": "d77e52da-afea-4048-cb3f-c38092c12151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "from datasets import ClassLabel, Value, Sequence, Features\n",
        "\n",
        "# Assuming `dataset` is your original dataset\n",
        "def convert_format(example):\n",
        "    # example[''] = str(example['id'])  # Convert id to string\n",
        "    example['pos_tags'] = [ClassLabel(names=['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']).str2int(tag) for tag in example['pos_tags']]\n",
        "    example['ner_tags'] = [ClassLabel(names=['B-O', 'B-AC', 'B-LF', 'I-LF']).str2int(tag) for tag in example['ner_tags']]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxkNzLq05Nag"
      },
      "outputs": [],
      "source": [
        "# Define the new features\n",
        "new_features = Features({\n",
        "    # 'id': Value(dtype='string', id=None),\n",
        "    'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
        "    'pos_tags': Sequence(feature=ClassLabel(names=['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']), length=-1, id=None),\n",
        "    'ner_tags': Sequence(feature=ClassLabel(names=['B-O', 'B-AC', 'B-LF', 'I-LF']), length=-1, id=None)\n",
        "})\n",
        "\n",
        "new_dataset = ds.map(convert_format, features=new_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNWp0yHf5Sjw"
      },
      "outputs": [],
      "source": [
        "label_list = new_dataset[\"train\"].features[f\"ner_tags\"].feature.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqGJduCC5UeG"
      },
      "outputs": [],
      "source": [
        "label_all_tokens = True\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBuC3m0P5bWm"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-CSL-lC5h0Q"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification, AutoTokenizer, Trainer\n",
        "import accelerate\n",
        "ds = load_dataset(dataset_name)\n",
        "new_dataset = ds.map(convert_format, features=new_features)\n",
        "\n",
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "batch_size = 4\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# tokenized_datasets = new_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_datasets = new_dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=new_dataset[\"train\"].column_names,\n",
        ")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer) #pads all of them to be of the same size\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "print(\"id2label\", id2label)\n",
        "print(\"label2id\", label2id)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"bert-finetuned-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    pust_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daM57uNs5i1u"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OZ24iV35nrA"
      },
      "outputs": [],
      "source": [
        "text = ds[\"test\"][6][\"tokens\"]\n",
        "labels = ds[\"test\"][6][\"ner_tags\"]\n",
        "model.to(DEVICE)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\",  truncation=True, is_split_into_words=True, return_offsets_mapping=True)\n",
        "inputs = {key: value.to(DEVICE) for key, value in inputs.items()}\n",
        "offset_mapping = inputs['offset_mapping'].cpu().tolist()[0]\n",
        "del inputs[\"offset_mapping\"]\n",
        "\n",
        "predictions = 0\n",
        "# Example forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    # Move the tensor to the CPU if it's on the GPU\n",
        "    pre_predictions = predictions\n",
        "    predictions = predictions.cpu().view(-1).tolist()\n",
        "\n",
        "input_ids = inputs['input_ids'].cpu().view(-1).tolist()\n",
        "attention_mask = inputs['attention_mask'].cpu().view(-1).tolist()\n",
        "\n",
        "aggregated_predictions = []\n",
        "for i, offset in enumerate(offset_mapping):\n",
        "  # Ignore special tokens\n",
        "  if sum(offset) == 0:\n",
        "    continue\n",
        "  # If the offset's start position is 0, it's a new word\n",
        "  if offset[0] == 0:\n",
        "    aggregated_predictions.append(predictions[i])\n",
        "\n",
        "\n",
        "print(len(aggregated_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3QikxJZ5oQN"
      },
      "outputs": [],
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "\n",
        "#prediction was what model predicted and labels is what is actually is.\n",
        "# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "# Compute multiple metrics on the test restuls\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
